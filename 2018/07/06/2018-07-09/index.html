<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"/>

<link rel="stylesheet" href="/css/main.css?v=6.7.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="[toc] Hive 笔试题考试时间：   姓名：__ 考试成绩：__   考试时长：180 分钟 注意事项：  自主答题，不能参考任何除本试卷外的其它资料。  总成绩共 200 分，共 20 题，每题 10 分，注意条理清楚、简明扼要、重点突出。   1.  Hive 的架构设计与运行流程，及其各模块的主要作用是什么，请画出架构图Hive是基于Hadoop的数据仓库，使用HQL作为查询接口、HD">
<meta name="keywords" content="bigdata,hive">
<meta property="og:type" content="article">
<meta property="og:title" content="Hive 笔试题">
<meta property="og:url" content="http://aaron-codes.github.io/2018/07/06/2018-07-09/index.html">
<meta property="og:site_name" content="Aaron&#39;s Blog">
<meta property="og:description" content="[toc] Hive 笔试题考试时间：   姓名：__ 考试成绩：__   考试时长：180 分钟 注意事项：  自主答题，不能参考任何除本试卷外的其它资料。  总成绩共 200 分，共 20 题，每题 10 分，注意条理清楚、简明扼要、重点突出。   1.  Hive 的架构设计与运行流程，及其各模块的主要作用是什么，请画出架构图Hive是基于Hadoop的数据仓库，使用HQL作为查询接口、HD">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://7xigvj.com1.z0.glb.clouddn.com/15311190753401.jpg">
<meta property="og:image" content="http://7xigvj.com1.z0.glb.clouddn.com/15311191181571.jpg">
<meta property="og:image" content="http://7xigvj.com1.z0.glb.clouddn.com/15311306228352.jpg">
<meta property="og:image" content="http://7xigvj.com1.z0.glb.clouddn.com/15311497429809.jpg">
<meta property="og:image" content="http://7xigvj.com1.z0.glb.clouddn.com/15311497630184.jpg">
<meta property="og:image" content="http://7xigvj.com1.z0.glb.clouddn.com/15311498682475.jpg">
<meta property="og:updated_time" content="2018-07-10T06:48:24.805Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hive 笔试题">
<meta name="twitter:description" content="[toc] Hive 笔试题考试时间：   姓名：__ 考试成绩：__   考试时长：180 分钟 注意事项：  自主答题，不能参考任何除本试卷外的其它资料。  总成绩共 200 分，共 20 题，每题 10 分，注意条理清楚、简明扼要、重点突出。   1.  Hive 的架构设计与运行流程，及其各模块的主要作用是什么，请画出架构图Hive是基于Hadoop的数据仓库，使用HQL作为查询接口、HD">
<meta name="twitter:image" content="http://7xigvj.com1.z0.glb.clouddn.com/15311190753401.jpg">



  <link rel="alternate" href="/atom.xml" title="Aaron's Blog" type="application/atom+xml"/>




  <link rel="canonical" href="http://aaron-codes.github.io/2018/07/06/2018-07-09/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Hive 笔试题 | Aaron's Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Aaron's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">人必有痴，而后有成。</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br/>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-schedule">

    
    
    
      
    

    

    <a href="/schedule/" rel="section"><i class="menu-item-icon fa fa-fw fa-calendar"></i> <br/>日程表</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-sitemap">

    
    
    
      
    

    

    <a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br/>站点地图</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://aaron-codes.github.io/2018/07/06/2018-07-09/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Aaron OuYang"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="https://avatars0.githubusercontent.com/u/3776267"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aaron's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Hive 笔试题

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-07-06 14:09:10" itemprop="dateCreated datePublished" datetime="2018-07-06T14:09:10+08:00">2018-07-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-07-10 14:48:24" itemprop="dateModified" datetime="2018-07-10T14:48:24+08:00">2018-07-10</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>[toc]</p>
<h1 id="Hive_笔试题">Hive 笔试题</h1><p>考试时间：   姓名：<strong><strong><em>__</em></strong></strong></p>
<p>考试成绩：<strong><strong><em>__</em></strong></strong>   考试时长：180 分钟</p>
<p>注意事项：</p>
<ol>
<li><p>自主答题，不能参考任何除本试卷外的其它资料。</p>
</li>
<li><p>总成绩共 200 分，共 20 题，每题 10 分，注意条理清楚、简明扼要、重点突出。</p>
</li>
</ol>
<h2 id="1-_Hive_的架构设计与运行流程，及其各模块的主要作用是什么，请画出架构图">1.  Hive 的架构设计与运行流程，及其各模块的主要作用是什么，请画出架构图</h2><p>Hive是基于Hadoop的数据仓库，使用HQL作为查询接口、HDFS作为存储底层、mapReduce作为执行层， 基于Hadoop平台解决了企业数据仓库构建的核心技术问题，证明了Hadoop平台的强大。从而进一步降低了Hadoop使用的准入门槛。</p>
<p><img src="http://7xigvj.com1.z0.glb.clouddn.com/15311190753401.jpg" alt=""></p>
<ul>
<li>用户接口主要有三个：CLI命令行，Client 和 Web UI</li>
<li>metaStore: hive 的元数据结构描述信息库，可选用不同的关系型数据库来存储，通过配置文件修改、查看数据库配置信息</li>
<li>Driver: hive核心驱动器接口类，衔接UI与内核的解析、优化、执行器的桥梁，生成的查询计划存储在HDFS中，并在随后由MapReduce调用执行</li>
</ul>
<p><img src="http://7xigvj.com1.z0.glb.clouddn.com/15311191181571.jpg" alt=""></p>
<h2 id="2-_Hive_的数据模型组成，及各组成模块的应用场景，请简要描述">2. Hive 的数据模型组成，及各组成模块的应用场景，请简要描述</h2><p>Hive 中所有的数据都存储在 HDFS 中， Hive 中包含以下数据模型： Table ， External Table ， Partition ， Bucket 。</p>
<ul>
<li>Hive 中的 Table 和数据库中的 Table 在概念上是类似的，每一个 Table 在 Hive 中都有一个相应的目录存储数据。例如，一个表 pvs ，它在 HDFS 中的路径为： /wh/pvs ，其中， wh 是在 hive-site.xml 中由 ${hive.metastore.warehouse.dir} 指定的数据仓库的目录，所有的 Table 数据（不包括 External Table ）都保存在这个目录中。</li>
<li>Partition 对应于数据库中的 Partition 列的密集索引，但是 Hive 中 Partition 的组织方式和数据库中的很不相同。在 Hive 中，表中的一个 Partition 对应于表下的一个目录，所有的 Partition 的数据都存储在对应的目录中。例如： pvs 表中包含 ds 和 city 两个 Partition ，则对应于 ds = 20090801, ctry = US 的 HDFS 子目录为： /wh/pvs/ds=20090801/ctry=US ；对应于 ds = 20090801, ctry = CA 的 HDFS 子目录为；/wh/pvs/ds=20090801/ctry=CA</li>
<li>Buckets 对指定列计算 hash ，根据 hash 值切分数据，目的是为了并行，每一个 Bucket 对应一个文件。将 user 列分散至 32 个 bucket ，首先对 user 列的值计算 hash ，对应 hash 值为 0 的 HDFS 目录为： /wh/pvs/ds=20090801/ctry=US/part-00000 ； hash 值为 20 的 HDFS 目录为： /wh/pvs/ds=20090801/ctry=US/part-00020</li>
<li>External Table 指向已经在 HDFS 中存在的数据，可以创建 Partition 。它和 Table 在元数据的组织上是相同的，而实际数据的存储则有较大的差异。</li>
<li>Table 的创建过程和数据加载过程（这两个过程可以在同一个语句中完成），在加载数据的过程中，实际数据会被移动到数据仓库目录中；之后对数据对访问将会直接在数据仓库目录中完成。删除表时，表中的数据和元数据将会被同时删除。</li>
<li>External Table 只有一个过程，加载数据和创建表同时完成（ CREATE EXTERNAL TABLE ……LOCATION ），实际数据是存储在 LOCATION 后面指定的 HDFS 路径中，并不会移动到数据仓库目录中。当删除一个 External Table 时，仅删除元信息。</li>
</ul>
<h2 id="3-_Hive_支持的文件格式和压缩格式，及其各自的特点？">3.  Hive 支持的文件格式和压缩格式，及其各自的特点？</h2><ul>
<li>存储格式，hive默认支持三种从存储格式，当然用户根据需要可以自定义存储格式。</li>
</ul>
<table>
<thead>
<tr>
<th>存储格式</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>SEQUENCEFILE</td>
<td>可压缩可分割二进制文件</td>
<td>需要一个合并文件的过程，且合并后的文件将不方便查看</td>
</tr>
<tr>
<td>TEXTFILE</td>
<td>简单方便查看</td>
<td>-</td>
</tr>
<tr>
<td>RCFILE</td>
<td>按列查找，理论上对于宽表采用这个比较好</td>
<td>比较新,还没有广泛应用</td>
</tr>
</tbody>
</table>
<ul>
<li>压缩格式</li>
</ul>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>工具</th>
<th>算法</th>
<th>文件扩展名</th>
<th>多文件</th>
<th>可分割性</th>
</tr>
</thead>
<tbody>
<tr>
<td>DEFLATE*</td>
<td>无</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>不</td>
<td>不</td>
</tr>
<tr>
<td>Gzip</td>
<td>gzip</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>不</td>
<td>不</td>
</tr>
<tr>
<td>ZIP</td>
<td>zip</td>
<td>DEFLATE</td>
<td>.zip</td>
<td>是</td>
<td>是，在文件范围内</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>不</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>lzop</td>
<td>LZO</td>
<td>.lzo</td>
<td>不</td>
<td>不</td>
</tr>
</tbody>
</table>
<ul>
<li>例子</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CREATE EXTERNAL TABLE src( key string, value string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;5&apos; LINES TERMINATED BY &apos;\n&apos;STORED AS RCFileALTER TABLE src SET SERDEPROPERTIES (&apos;serialization.null.format&apos;=&apos;&apos;);</span><br><span class="line">set mapred.reduce.tasks=200;</span><br><span class="line">set io.sort.mb=500;</span><br><span class="line">set mapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"></span><br><span class="line">INSERT OVERWRITE TABLE src_bzip2SELECT * FROM src SORT BY key;</span><br></pre></td></tr></table></figure>
<h2 id="4-_Hive_内外表的区分方法，及内外表的差异点？">4.  Hive 内外表的区分方法，及内外表的差异点？</h2><ul>
<li><p>异同点：</p>
<ol>
<li>创建表的语句上外部表会有一个 external 的关键字。</li>
<li>删除表的时候，内部表不仅会删除元数据，还会会将hdfs上对应的目录删除掉，而外部表只是删除了元数据，并不会删除 hdfs 上的数据</li>
</ol>
</li>
</ul>
<h2 id="5-_Hive_视图如何创建，视图有什么特点，及其应用场景？">5. Hive 视图如何创建，视图有什么特点，及其应用场景？</h2><p>Hive中的视图和关系型数据库一样，Hive中也提供了视图的功能，注意Hive中视图的特性，和关系型数据库中的稍有区别：</p>
<p>只有逻辑视图，没有物化视图；<br>视图只能查询，不能Load/Insert/Update/Delete数据；<br>视图在创建时候，只是保存了一份元数据，当查询视图的时候，才开始执行视图对应的那些子查询；</p>
<h2 id="6-_Hive_常用的_12_个命令，及其作用">6. Hive 常用的 12 个命令，及其作用</h2><ul>
<li>创建表：</li>
</ul>
<p>hive&gt; CREATE TABLE pokes (foo INT, bar STRING);<br>        Creates a table called pokes with two columns, the first being an integer and the other a string</p>
<ul>
<li>创建一个新表，结构与其他一样</li>
</ul>
<p>hive&gt; create table new_table like records;</p>
<ul>
<li>创建分区表：</li>
</ul>
<p>hive&gt; create table logs(ts bigint,line string) partitioned by (dt String,country String);</p>
<ul>
<li>加载分区表数据：</li>
</ul>
<p>hive&gt; load data local inpath ‘/home/hadoop/input/hive/partitions/file1’ into table logs partition (dt=’2001-01-01’,country=’GB’);</p>
<ul>
<li><p>展示表中有多少分区：<br>hive&gt; show partitions logs;</p>
</li>
<li><p>展示所有表：</p>
</li>
</ul>
<p>hive&gt; SHOW TABLES;<br>        lists all the tables<br>hive&gt; SHOW TABLES ‘.*s’;</p>
<p>lists all the table that end with ‘s’. The pattern matching follows Java regular<br>expressions. Check out this link for documentation <a href="http://java.sun.com/javase/6/docs/api/java/util/regex/Pattern.html" target="_blank" rel="noopener">http://java.sun.com/javase/6/docs/api/java/util/regex/Pattern.html</a></p>
<ul>
<li>显示表的结构信息</li>
</ul>
<p>hive&gt; DESCRIBE invites;<br>        shows the list of columns</p>
<ul>
<li>更新表的名称：</li>
</ul>
<p>hive&gt; ALTER TABLE source RENAME TO target;</p>
<ul>
<li>添加新一列</li>
</ul>
<p>hive&gt; ALTER TABLE invites ADD COLUMNS (new_col2 INT COMMENT ‘a comment’);</p>
<ul>
<li>删除表：</li>
</ul>
<p>hive&gt; DROP TABLE records;</p>
<ul>
<li>删除表中数据，但要保持表的结构定义</li>
</ul>
<p>hive&gt; dfs -rmr /user/hive/warehouse/records;</p>
<ul>
<li>从本地文件加载数据：</li>
</ul>
<p>hive&gt; LOAD DATA LOCAL INPATH ‘/home/hadoop/input/ncdc/micro-tab/sample.txt’ OVERWRITE INTO TABLE records;</p>
<ul>
<li>显示所有函数：</li>
</ul>
<p>hive&gt; show functions;</p>
<ul>
<li>查看函数用法：</li>
</ul>
<p>hive&gt; describe function substr;</p>
<ul>
<li>查看数组、map、结构</li>
</ul>
<p>hive&gt; select col1[0],col2[‘b’],col3.c from complex;</p>
<ul>
<li>内连接：</li>
</ul>
<p>hive&gt; SELECT sales.<em>, things.</em> FROM sales JOIN things ON (sales.id = things.id);</p>
<ul>
<li>查看hive为某个查询使用多少个MapReduce作业</li>
</ul>
<p>hive&gt; Explain SELECT sales.<em>, things.</em> FROM sales JOIN things ON (sales.id = things.id);</p>
<ul>
<li>外连接：</li>
</ul>
<p>hive&gt; SELECT sales.<em>, things.</em> FROM sales LEFT OUTER JOIN things ON (sales.id = things.id);<br>hive&gt; SELECT sales.<em>, things.</em> FROM sales RIGHT OUTER JOIN things ON (sales.id = things.id);<br>hive&gt; SELECT sales.<em>, things.</em> FROM sales FULL OUTER JOIN things ON (sales.id = things.id);</p>
<ul>
<li>in查询：Hive不支持，但可以使用LEFT SEMI JOIN</li>
</ul>
<p>hive&gt; SELECT * FROM things LEFT SEMI JOIN sales ON (sales.id = things.id);</p>
<ul>
<li>Map连接：Hive可以把较小的表放入每个Mapper的内存来执行连接操作</li>
</ul>
<p>hive&gt; SELECT /<em>+ MAPJOIN(things) </em>/ sales.<em>, things.</em> FROM sales JOIN things ON (sales.id = things.id);</p>
<p>INSERT OVERWRITE TABLE ..SELECT：新表预先存在<br>hive&gt; FROM records2</p>
<pre><code>&gt; INSERT OVERWRITE TABLE stations_by_year <span class="keyword">SELECT</span> year, COUNT(<span class="keyword">DISTINCT</span> station) <span class="keyword">GROUP</span> <span class="keyword">BY</span> year 
&gt; INSERT OVERWRITE TABLE records_by_year <span class="keyword">SELECT</span> year, COUNT(<span class="number">1</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> year
&gt; INSERT OVERWRITE TABLE good_records_by_year <span class="keyword">SELECT</span> year, COUNT(<span class="number">1</span>) <span class="keyword">WHERE</span> temperature != <span class="number">9999</span> <span class="keyword">AND</span> (quality = <span class="number">0</span> <span class="keyword">OR</span> quality = <span class="number">1</span> <span class="keyword">OR</span> quality = <span class="number">4</span> <span class="keyword">OR</span> quality = <span class="number">5</span> <span class="keyword">OR</span> quality = <span class="number">9</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> year;  
</code></pre><p>CREATE TABLE … AS SELECT：新表表预先不存在<br>hive&gt;CREATE TABLE target AS SELECT col1,col2 FROM source;</p>
<ul>
<li>创建视图：</li>
</ul>
<p>hive&gt; CREATE VIEW valid_records AS SELECT * FROM records2 WHERE temperature !=9999;</p>
<ul>
<li>查看视图详细信息：</li>
</ul>
<p>hive&gt; DESCRIBE EXTENDED valid_records;</p>
<h2 id="7-_Hive_常用的_10_个系统函数，及其作用">7. Hive 常用的 10 个系统函数，及其作用</h2><p><a href="https://www.iteblog.com/archives/1639.html" target="_blank" rel="noopener">Hive常用字符串函数</a></p>
<p>Hive分析窗口函数(一) SUM,AVG,MIN,MAX<br>Hive分析窗口函数(二) NTILE,ROW_NUMBER,RANK,DENSE_RANK<br>Hive分析窗口函数(三) CUME_DIST,PERCENT_RANK<br>Hive分析窗口函数(四) LAG,LEAD,FIRST_VALUE,LAST_VALUE<br>Hive分析窗口函数(五) GROUPING SETS,GROUPING__ID,CUBE,ROLLUP</p>
<h2 id="8-_请详细描述将一个有结构的文本文件_student-txt_导入到一个_Hive_表当中的步骤，及其关键字？">8. 请详细描述将一个有结构的文本文件 student.txt 导入到一个 Hive 表当中的步骤，及其关键字？</h2><p>一、从本地文件系统中导入数据到Hive表</p>
<ul>
<li>先在Hive里面创建好表，如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table wyp</span><br><span class="line">    &gt; (id int, name string,</span><br><span class="line">    &gt; age int, tel string)</span><br><span class="line">    &gt; ROW FORMAT DELIMITED</span><br><span class="line">    &gt; FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">    &gt; STORED AS TEXTFILE;</span><br><span class="line">OK</span><br><span class="line">Time taken: 2.832 seconds</span><br></pre></td></tr></table></figure>
<ul>
<li>这个表很简单，只有四个字段，具体含义我就不解释了。本地文件系统里面有个 /home/wyp/wyp.txt 文件，内容如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[wyp@master ~]$ cat wyp.txt</span><br><span class="line">1       wyp     25      13188888888888</span><br><span class="line">2       test    30      13888888888888</span><br><span class="line">3       zs      34      899314121</span><br></pre></td></tr></table></figure>
<ul>
<li>wyp.txt文件中的数据列之间是使用 \t 分割的，可以通过下面的语句将这个文件里面的数据导入到wyp表里面，操作如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath &apos;wyp.txt&apos; into table wyp;</span><br><span class="line">Copying data from file:/home/wyp/wyp.txt</span><br><span class="line">Copying file: file:/home/wyp/wyp.txt</span><br><span class="line">Loading data to table default.wyp</span><br><span class="line">Table default.wyp stats:</span><br><span class="line">[num_partitions: 0, num_files: 1, num_rows: 0, total_size: 67]</span><br><span class="line">OK</span><br><span class="line">Time taken: 5.967 seconds</span><br></pre></td></tr></table></figure>
<h2 id="9-_请简述_udf/udaf/udtf_是什么，各自解决的问题，及典型代表应用场景。">9. 请简述 udf/udaf/udtf 是什么，各自解决的问题，及典型代表应用场景。</h2><p>Hive自定义函数包括三种UDF、UDAF、UDTF</p>
<ul>
<li><p>UDF(User-Defined-Function) 一进一出</p>
</li>
<li><p>UDAF(User- Defined Aggregation Funcation) 聚集函数，多进一出。Count/max/min</p>
</li>
<li><p>UDTF(User-Defined Table-Generating Functions)  一进多出，如lateral view explore()</p>
</li>
</ul>
<p>使用方式 ：在HIVE会话中add 自定义函数的jar文件，然后创建function继而使用函数</p>
<h2 id="10-_udaf_的实现步骤，及其包含的主要方法，及每个方法要解决的问题，并写代码自实现聚合函数_max_函数？">10. udaf 的实现步骤，及其包含的主要方法，及每个方法要解决的问题，并写代码自实现聚合函数 max 函数？</h2><p>多行进一行出，如sum()、min()，用在group  by时</p>
<ol>
<li>必须继承</li>
</ol>
<p>　　org.apache.hadoop.hive.ql.exec.UDAF(函数类继承)</p>
<p>　　org.apache.hadoop.hive.ql.exec.UDAFEvaluator(内部类Evaluator实现UDAFEvaluator接口)</p>
<ol>
<li>Evaluator需要实现 init、iterate、terminatePartial、merge、terminate这几个函数</li>
</ol>
<p>　　init():类似于构造函数，用于UDAF的初始化</p>
<p>　　iterate():接收传入的参数，并进行内部的轮转，返回boolean</p>
<p>　　terminatePartial():无参数，其为iterate函数轮转结束后，返回轮转数据，类似于hadoop的Combiner</p>
<p>　　merge():接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean</p>
<p>　　terminate():返回最终的聚集函数结果</p>
<h2 id="11、_hive_设置参数的方法有哪些？并列举_8_个常用的参数设置？">11、 hive 设置参数的方法有哪些？并列举 8 个常用的参数设置？</h2><p>Hive提供三种可以改变环境变量的方法，分别是：<br>（1）、修改${HIVE_HOME}/conf/hive-site.xml配置文件；<br>（2）、命令行参数；<br>（3）、在已经进入cli时进行参数声明。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br></pre></td><td class="code"><pre><span class="line">hive.ddl.output.format：hive的ddl语句的输出格式，默认是text，纯文本，还有json格式，这个是0.90以后才出的新配置；</span><br><span class="line"></span><br><span class="line">hive.exec.script.wrapper：hive调用脚本时的包装器，默认是null，如果设置为python的话，那么在做脚本调用操作时语句会变为python &lt;script command&gt;，null的话就是直接执行&lt;script command&gt;；</span><br><span class="line"></span><br><span class="line">hive.exec.plan：hive执行计划的文件路径，默认是null，会在运行时自动设置，形如hdfs://xxxx/xxx/xx；</span><br><span class="line"></span><br><span class="line">hive.exec.scratchdir：hive用来存储不同阶段的map/reduce的执行计划的目录，同时也存储中间输出结果，默认是/tmp/&lt;user.name&gt;/hive，我们实际一般会按组区分，然后组内自建一个tmp目录存储；</span><br><span class="line"></span><br><span class="line">hive.exec.submitviachild：在非local模式下，决定hive是否要在独立的jvm中执行map/reduce；默认是false，也就是说默认map/reduce的作业是在hive的jvm上去提交的；</span><br><span class="line"></span><br><span class="line">hive.exec.script.maxerrsize：当用户调用transform或者map或者reduce执行脚本时，最大的序列化错误数，默认100000，一般也不用修改；</span><br><span class="line"></span><br><span class="line">hive.exec.compress.output：一个查询的最后一个map/reduce任务输出是否被压缩的标志，默认为false，但是一般会开启为true，好处的话，节省空间不说，在不考虑cpu压力的时候会提高io；</span><br><span class="line"></span><br><span class="line">hive.exec.compress.intermediate：类似上个，在一个查询的中间的map/reduce任务输出是否要被压缩，默认false，</span><br><span class="line"></span><br><span class="line">hive.jar.path：当使用独立的jvm提交作业时，hive_cli.jar所在的位置，无默认值；</span><br><span class="line"></span><br><span class="line">hive.aux.jars.path：当用户自定义了UDF或者SerDe，这些插件的jar都要放到这个目录下，无默认值；</span><br><span class="line"></span><br><span class="line">hive.partition.pruning：在编译器发现一个query语句中使用分区表然而未提供任何分区谓词做查询时，抛出一个错误从而保护分区表，默认是nonstrict；（待读源码后细化，网上资料极少）</span><br><span class="line"></span><br><span class="line">hive.map.aggr：map端聚合是否开启，默认开启；</span><br><span class="line"></span><br><span class="line">hive.join.emit.interval：在发出join结果之前对join最右操作缓存多少行的设定，默认1000；hive jira里有个对该值设置太小的bugfix；</span><br><span class="line"></span><br><span class="line">hive.map.aggr.hash.percentmemory：map端聚合时hash表所占用的内存比例，默认0.5，这个在map端聚合开启后使用，</span><br><span class="line"></span><br><span class="line">hive.default.fileformat：CREATE TABLE语句的默认文件格式，默认TextFile，其他可选的有SequenceFile、RCFile还有Orc；</span><br><span class="line"></span><br><span class="line">hive.merge.mapfiles：在只有map的作业结束时合并小文件，默认开启true；</span><br><span class="line"></span><br><span class="line">hive.merge.mapredfiles：在一个map/reduce作业结束后合并小文件，默认不开启false；</span><br><span class="line"></span><br><span class="line">hive.merge.size.per.task：作业结束时合并文件的大小，默认256MB；</span><br><span class="line"></span><br><span class="line">hive.merge.smallfiles.avgsize：在作业输出文件小于该值时，起一个额外的map/reduce作业将小文件合并为大文件，小文件的基本阈值，设置大点可以减少小文件个数，需要mapfiles和mapredfiles为true，默认值是16MB；</span><br><span class="line"></span><br><span class="line">mapred.reduce.tasks：每个作业的reduce任务数，默认是hadoop client的配置1个；</span><br><span class="line"></span><br><span class="line">hive.exec.reducers.bytes.per.reducer：每个reducer的大小，默认是1G，输入文件如果是10G，那么就会起10个reducer；</span><br><span class="line"></span><br><span class="line">hive.exec.reducers.max：reducer的最大个数，如果在mapred.reduce.tasks设置为负值，那么hive将取该值作为reducers的最大可能值。当然还要依赖（输入文件大小/hive.exec.reducers.bytes.per.reducer）所得出的大小，取其小值作为reducer的个数，hive默认是999；</span><br><span class="line"></span><br><span class="line">hive.fileformat.check：加载数据文件时是否校验文件格式，默认是true；</span><br><span class="line"></span><br><span class="line">hive.groupby.skewindata：group by操作是否允许数据倾斜，默认是false，当设置为true时，执行计划会生成两个map/reduce作业，第一个MR中会将map的结果随机分布到reduce中，达到负载均衡的目的来解决数据倾斜，</span><br><span class="line"></span><br><span class="line">hive.groupby.mapaggr.checkinterval：map端做聚合时，group by 的key所允许的数据行数，超过该值则进行分拆，默认是100000；</span><br><span class="line"></span><br><span class="line">hive.mapred.local.mem：本地模式时，map/reduce的内存使用量，默认是0，就是无限制；</span><br><span class="line"></span><br><span class="line">hive.mapjoin.followby.map.aggr.hash.percentmemory：map端聚合时hash表的内存占比，该设置约束group by在map join后进行，否则使用hive.map.aggr.hash.percentmemory来确认内存占比，默认值0.3；</span><br><span class="line"></span><br><span class="line">hive.map.aggr.hash.force.flush.memeory.threshold：map端聚合时hash表的最大可用内存，如果超过该值则进行flush数据，默认是0.9；</span><br><span class="line"></span><br><span class="line">hive.map.aggr.hash.min.reduction：如果hash表的容量与输入行数之比超过这个数，那么map端的hash聚合将被关闭，默认是0.5，设置为1可以保证hash聚合永不被关闭；</span><br><span class="line"></span><br><span class="line">hive.optimize.groupby：在做分区和表查询时是否做分桶group by，默认开启true；</span><br><span class="line"></span><br><span class="line">hive.multigroupby.singlemr：将多个group by产出为一个单一map/reduce任务计划，当然约束前提是group by有相同的key，默认是false；</span><br><span class="line"></span><br><span class="line">hive.optimize.cp：列裁剪，默认开启true，在做查询时只读取用到的列，这个是个有用的优化；</span><br><span class="line"></span><br><span class="line">hive.optimize.index.filter：自动使用索引，默认不开启false；</span><br><span class="line"></span><br><span class="line">hive.optimize.index.groupby：是否使用聚集索引优化group-by查询，默认关闭false；</span><br><span class="line"></span><br><span class="line">hive.optimize.ppd：是否支持谓词下推，默认开启；所谓谓词下推，将外层查询块的 WHERE 子句中的谓词移入所包含的较低层查询块（例如视图），从而能够提早进行数据过滤以及有可能更好地利用索引。</span><br><span class="line"></span><br><span class="line">hive.optimize.ppd.storage：谓词下推开启时，谓词是否下推到存储handler，默认开启，在谓词下推关闭时不起作用；</span><br><span class="line"></span><br><span class="line">hive.ppd.recognizetransivity：在等值join条件下是否产地重复的谓词过滤器，默认开启；</span><br><span class="line"></span><br><span class="line">hive.join.cache.size：在做表join时缓存在内存中的行数，默认25000；</span><br><span class="line"></span><br><span class="line">hive.mapjoin.bucket.cache.size：mapjoin时内存cache的每个key要存储多少个value，默认100；</span><br><span class="line"></span><br><span class="line">hive.optimize.skewjoin：是否开启数据倾斜的join优化，默认不开启false；</span><br><span class="line"></span><br><span class="line">hive.skewjoin.key：判断数据倾斜的阈值，如果在join中发现同样的key超过该值则认为是该key是倾斜的join key，默认是100000；</span><br><span class="line"></span><br><span class="line">hive.skewjoin.mapjoin.map.tasks：在数据倾斜join时map join的map数控制，默认是10000；</span><br><span class="line"></span><br><span class="line">hive.skewjoin.mapjoin.min.split：数据倾斜join时map join的map任务的最小split大小，默认是33554432，该参数要结合上面的参数共同使用来进行细粒度的控制；</span><br><span class="line"></span><br><span class="line">hive.mapred.mode：hive操作执行时的模式，默认是nonstrict非严格模式，如果是strict模式，很多有风险的查询会被禁止运行，比如笛卡尔积的join和动态分区；</span><br><span class="line"></span><br><span class="line">hive.exec.script.maxerrsize：一个map/reduce任务允许打印到标准错误里的最大字节数，为了防止脚本把分区日志填满，默认是100000；</span><br><span class="line"></span><br><span class="line">hive.exec.script.allow.partial.consumption：hive是否允许脚本不从标准输入中读取任何内容就成功退出，默认关闭false；</span><br><span class="line"></span><br><span class="line">hive.script.operator.id.env.var：在用户使用transform函数做自定义map/reduce时，存储唯一的脚本标识的环境变量的名字，默认HIVE_SCRIPT_OPERATOR_ID；</span><br><span class="line"></span><br><span class="line">hive.exec.compress.output：控制hive的查询结果输出是否进行压缩，压缩方式在hadoop的mapred.output.compress中配置，默认不压缩false；</span><br><span class="line"></span><br><span class="line">hive.exec.compress.intermediate：控制hive的查询中间结果是否进行压缩，同上条配置，默认不压缩false；</span><br><span class="line"></span><br><span class="line">hive.exec.parallel：hive的执行job是否并行执行，默认不开启false，在很多操作如join时，子查询之间并无关联可独立运行，这种情况下开启并行运算可以大大加速；</span><br><span class="line"></span><br><span class="line">hvie.exec.parallel.thread.number：并行运算开启时，允许多少作业同时计算，默认是8；</span><br><span class="line"></span><br><span class="line">hive.exec.rowoffset：是否提供行偏移量的虚拟列，默认是false不提供，Hive有两个虚拟列:一个是INPUT__FILE__NAME,表示输入文件的路径，另外一个是BLOCK__OFFSET__INSIDE__FILE，表示记录在文件中的块偏移量，这对排查出现不符合预期或者null结果的查询是很有帮助的；</span><br><span class="line"></span><br><span class="line">hive.task.progress：控制hive是否在执行过程中周期性的更新任务进度计数器，开启这个配置可以帮助job tracker更好的监控任务的执行情况，但是会带来一定的性能损耗，当动态分区标志hive.exec.dynamic.partition开启时，本配置自动开启；</span><br><span class="line"></span><br><span class="line">hive.exec.pre.hooks：执行前置条件，一个用逗号分隔开的实现了org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext接口的java class列表，配置了该配置后，每个hive任务执行前都要执行这个执行前钩子，默认是空；</span><br><span class="line"></span><br><span class="line">hive.exec.post.hooks：同上，执行后钩子，默认是空；</span><br><span class="line"></span><br><span class="line">hive.exec.failure.hooks：同上，异常时钩子，在程序发生异常时执行，默认是空；</span><br><span class="line"></span><br><span class="line">hive.mergejob.maponly：试图生成一个只有map的任务去做merge，前提是支持CombineHiveInputFormat，默认开启true；</span><br><span class="line"></span><br><span class="line">hive.mapjoin.smalltable.filesize：输入表文件的mapjoin阈值，如果输入文件的大小小于该值，则试图将普通join转化为mapjoin，默认25MB；</span><br><span class="line"></span><br><span class="line">hive.mapjoin.localtask.max.memory.usage：mapjoin本地任务执行时hash表容纳key/value的最大量，超过这个值的话本地任务会自动退出，默认是0.9；</span><br><span class="line"></span><br><span class="line">hive.mapjoin.followby.gby.localtask.max.memory.usage：类似上面，只不过是如果mapjoin后有一个group by的话，该配置控制类似这样的query的本地内存容量上限，默认是0.55；</span><br><span class="line"></span><br><span class="line">hive.mapjoin.check.memory.rows：在运算了多少行后执行内存使用量检查，默认100000；</span><br><span class="line"></span><br><span class="line">hive.heartbeat.interval：发送心跳的时间间隔，在mapjoin和filter操作中使用，默认1000；</span><br><span class="line"></span><br><span class="line">hive.auto.convert.join：根据输入文件的大小决定是否将普通join转换为mapjoin的一种优化，默认不开启false；</span><br><span class="line"></span><br><span class="line">hive.script.auto.progress：hive的transform/map/reduce脚本执行时是否自动的将进度信息发送给TaskTracker来避免任务没有响应被误杀，本来是当脚本输出到标准错误时，发送进度信息，但是开启该项后，输出到标准错误也不会导致信息发送，因此有可能会造成脚本有死循环产生，但是TaskTracker却没有检查到从而一直循环下去；</span><br><span class="line"></span><br><span class="line">hive.script.serde：用户脚本转换输入到输出时的SerDe约束，默认是org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe；</span><br><span class="line"></span><br><span class="line">hive.script.recordreader：从脚本读数据的时候的默认reader，默认是org.apache.hadoop.hive.ql.exec.TextRecordReader；</span><br><span class="line"></span><br><span class="line">hive.script.recordwriter：写数据到脚本时的默认writer，默认org.apache.hadoop.hive.ql.exec.TextRecordWriter；</span><br><span class="line"></span><br><span class="line">hive.input.format：输入格式，默认是org.apache.hadoop.hive.ql.io.CombineHiveInputFormat，如果出现问题，可以改用org.apache.hadoop.hive.ql.io.HiveInputFormat；</span><br><span class="line"></span><br><span class="line">hive.udtf.auto.progress：UDTF执行时hive是否发送进度信息到TaskTracker，默认是false；</span><br><span class="line"></span><br><span class="line">hive.mapred.reduce.tasks.speculative.execution：reduce任务推测执行是否开启，默认是true；</span><br><span class="line"></span><br><span class="line">hive.exec.counters.pull.interval：运行中job轮询JobTracker的时间间隔，设置小会影响JobTracker的load，设置大可能看不出运行任务的信息，要去平衡，默认是1000；</span><br><span class="line"></span><br><span class="line">hive.enforce.bucketing：数据分桶是否被强制执行，默认false，如果开启，则写入table数据时会启动分桶，</span><br><span class="line"></span><br><span class="line">hive.enforce.sorting：开启强制排序时，插数据到表中会进行强制排序，默认false；</span><br><span class="line"></span><br><span class="line">hive.optimize.reducededuplication：如果数据已经根据相同的key做好聚合，那么去除掉多余的map/reduce作业，此配置是文档的推荐配置，建议打开，默认是true；</span><br><span class="line"></span><br><span class="line">hive.exec.dynamic.partition：在DML/DDL中是否支持动态分区，默认false；</span><br><span class="line"></span><br><span class="line">hive.exec.dynamic.partition.mode：默认strict，在strict模式下，动态分区的使用必须在一个静态分区确认的情况下，其他分区可以是动态；</span><br><span class="line"></span><br><span class="line">hive.exec.max.dynamic.partitions：动态分区的上限，默认1000；</span><br><span class="line"></span><br><span class="line">hive.exec.max.dynamic.partitions.pernode：每个mapper/reducer节点可以创建的最大动态分区数，默认100；</span><br><span class="line"></span><br><span class="line">hive.exec.max.created.files：一个mapreduce作业能创建的HDFS文件最大数，默认是100000；</span><br><span class="line"></span><br><span class="line">hive.exec.default.partition.name：当动态分区启用时，如果数据列里包含null或者空字符串的话，数据会被插入到这个分区，默认名字是HIVE_DEFAULT_PARTITION；</span><br><span class="line"></span><br><span class="line">hive.fetch.output.serde：FetchTask序列化fetch输出时需要的SerDe，默认是org.apache.hadoop.hive.serde2.DelimitedJSONSerDe;</span><br><span class="line"></span><br><span class="line">hive.exec.mode.local.auto：是否由hive决定自动在local模式下运行，默认是false，</span><br><span class="line"></span><br><span class="line">hive.exec.drop.ignorenoneexistent：在drop表或者视图时如果发现表或视图不存在，是否报错，默认是true；</span><br><span class="line"></span><br><span class="line">hive.exec.show.job.failure.debug.info：在作业失败时是否提供一个任务debug信息，默认true；</span><br><span class="line"></span><br><span class="line">hive.auto.progress.timeout：运行自动progressor的时间间隔，默认是0等价于forever；</span><br><span class="line"></span><br><span class="line">hive.table.parameters.default：新建表的属性字段默认值，默认是empty空；</span><br><span class="line"></span><br><span class="line">hive.variable.substitute：是否支持变量替换，如果开启的话，支持语法如varvar&#123;system:var&#125;和$&#123;env.var&#125;，默认是true；</span><br><span class="line"></span><br><span class="line">hive.error.on.empty.partition：在遇到结果为空的动态分区时是否报错，默认是false；</span><br><span class="line"></span><br><span class="line">hive.exim.uri.scheme.whitelist：在导入导出数据时提供的一个白名单列表，列表项之间由逗号分隔，默认hdfs,pfile；</span><br><span class="line"></span><br><span class="line">hive.limit.row.max.size：字面意思理解就是在使用limit做数据的子集查询时保证的最小行数据量，默认是100000；</span><br><span class="line"></span><br><span class="line">hive.limit.optimize.limit.file：使用简单limit查询数据子集时，可抽样的最大文件数，默认是10；</span><br><span class="line"></span><br><span class="line">hive.limit.optimize.enable：使用简单limit抽样数据时是否开启优化选项，默认是false，关于limit的优化问题，在hive programming书中解释的是这个feature有drawback，对于抽样的不确定性给出了风险提示；</span><br><span class="line"></span><br><span class="line">hive.limit.optimize.fetch.max：使用简单limit抽样数据允许的最大行数，默认50000，查询query受限，insert不受影响；</span><br><span class="line"></span><br><span class="line">hive.rework.mapredwork：是否重做mapreduce，默认是false；</span><br><span class="line"></span><br><span class="line">hive.sample.seednumber：用来区分抽样的数字，默认是0；</span><br><span class="line"></span><br><span class="line">hive.io.exception.handlers：io异常处理handler类列表，默认是空，当record reader发生io异常时，由这些handler来处理异常；</span><br><span class="line"></span><br><span class="line">hive.autogen.columnalias.prefix.label：当在执行中自动产生列别名的前缀，当类似count这样的聚合函数起作用时，如果不明确指出count(a) as xxx的话，那么默认会从列的位置的数字开始算起添加，比如第一个count的结果会冠以列名_c0，接下来依次类推，默认值是_c，数据开发过程中应该很多人都看到过这个别名；</span><br><span class="line"></span><br><span class="line">hive.autogen.columnalias.prefix.includefuncname：在自动生成列别名时是否带函数的名字，默认是false；</span><br><span class="line"></span><br><span class="line">hive.exec.perf.logger：负责记录客户端性能指标的日志类名，必须是org.apache.hadoop.hive.ql.log.PerfLogger的子类，默认是org.apache.hadoop.hive.ql.log.PerfLogger；</span><br><span class="line"></span><br><span class="line">hive.start.cleanup.scratchdir：当启动hive服务时是否清空hive的scratch目录，默认是false；</span><br><span class="line"></span><br><span class="line">hive.output.file.extension：输出文件扩展名，默认是空；</span><br><span class="line"></span><br><span class="line">hive.insert.into.multilevel.dirs：是否插入到多级目录，默认是false；</span><br><span class="line"></span><br><span class="line">hive.files.umask.value：hive创建文件夹时的dfs.umask值，默认是0002；</span><br><span class="line"></span><br><span class="line">hive.metastore.local：控制hive是否连接一个远程metastore服务器还是开启一个本地客户端jvm，默认是true，Hive0.10已经取消了该配置项；</span><br><span class="line"></span><br><span class="line">javax.jdo.option.ConnectionURL：JDBC连接字符串，默认jdbc:derby:;databaseName=metastore_db;create=true；</span><br><span class="line"></span><br><span class="line">javax.jdo.option.ConnectionDriverName：JDBC的driver，默认org.apache.derby.jdbc.EmbeddedDriver；</span><br><span class="line"></span><br><span class="line">javax.jdo.PersisteneManagerFactoryClass：实现JDO PersistenceManagerFactory的类名，默认org.datanucleus.jdo.JDOPersistenceManagerFactory；</span><br><span class="line"></span><br><span class="line">javax.jdo.option.DetachAllOnCommit：事务提交后detach所有提交的对象，默认是true；</span><br><span class="line"></span><br><span class="line">javax.jdo.option.NonTransactionalRead：是否允许非事务的读，默认是true；</span><br><span class="line"></span><br><span class="line">javax.jdo.option.ConnectionUserName：username，默认APP；</span><br><span class="line"></span><br><span class="line">javax.jdo.option.ConnectionPassword：password，默认mine；</span><br><span class="line"></span><br><span class="line">javax.jdo.option.Multithreaded：是否支持并发访问metastore，默认是true；</span><br><span class="line"></span><br><span class="line">datanucleus.connectionPoolingType：使用连接池来访问JDBC metastore，默认是DBCP；</span><br><span class="line"></span><br><span class="line">datanucleus.validateTables：检查是否存在表的schema，默认是false；</span><br><span class="line"></span><br><span class="line">datanucleus.validateColumns：检查是否存在列的schema，默认false；</span><br><span class="line"></span><br><span class="line">datanucleus.validateConstraints：检查是否存在constraint的schema，默认false；</span><br><span class="line"></span><br><span class="line">datanucleus.stroeManagerType：元数据存储类型，默认rdbms；</span><br><span class="line"></span><br><span class="line">datanucleus.autoCreateSchema：在不存在时是否自动创建必要的schema，默认是true；</span><br><span class="line"></span><br><span class="line">datanucleus.aotuStartMechanismMode：如果元数据表不正确，抛出异常，默认是checked；</span><br><span class="line"></span><br><span class="line">datanucleus.transactionIsolation：默认的事务隔离级别，默认是read-committed；</span><br><span class="line"></span><br><span class="line">datanucleus.cache.level2：使用二级缓存，默认是false；</span><br><span class="line"></span><br><span class="line">datanucleus.cache.level2.type：二级缓存的类型，有两种，SOFT:软引用，WEAK:弱引用，默认是SOFT；</span><br><span class="line"></span><br><span class="line">datanucleus.identifierFactory：id工厂生产表和列名的名字，默认是datanucleus；</span><br><span class="line"></span><br><span class="line">datanucleus.plugin.pluginRegistryBundleCheck：当plugin被发现并且重复时的行为，默认是LOG；</span><br><span class="line"></span><br><span class="line">hive.metastroe.warehouse.dir：数据仓库的位置，默认是/user/hive/warehouse；</span><br><span class="line"></span><br><span class="line">hive.metastore.execute.setugi：非安全模式，设置为true会令metastore以客户端的用户和组权限执行DFS操作，默认是false，这个属性需要服务端和客户端同时设置；</span><br><span class="line"></span><br><span class="line">hive.metastore.event.listeners：metastore的事件监听器列表，逗号隔开，默认是空；</span><br><span class="line"></span><br><span class="line">hive.metastore.partition.inherit.table.properties：当新建分区时自动继承的key列表，默认是空；</span><br><span class="line"></span><br><span class="line">hive.metastore.end.function.listeners：metastore函数执行结束时的监听器列表，默认是空；</span><br><span class="line"></span><br><span class="line">hive.metastore.event.expiry.duration：事件表中事件的过期时间，默认是0；</span><br><span class="line"></span><br><span class="line">hive.metastore.event.clean.freq：metastore中清理过期事件的定时器的运行周期，默认是0；</span><br><span class="line"></span><br><span class="line">hive.metastore.connect.retries：创建metastore连接时的重试次数，默认是5；</span><br><span class="line"></span><br><span class="line">hive.metastore.client.connect.retry.delay：客户端在连续的重试连接等待的时间，默认1；</span><br><span class="line"></span><br><span class="line">hive.metastore.client.socket.timeout：客户端socket超时时间，默认20秒；</span><br><span class="line"></span><br><span class="line">hive.metastore.rawstore.impl：原始metastore的存储实现类，默认是org.apache.hadoop.hive.metastore.ObjectStore；</span><br><span class="line"></span><br><span class="line">hive.metastore.batch.retrieve.max：在一个batch获取中，能从metastore里取出的最大记录数，默认是300；</span><br><span class="line"></span><br><span class="line">hive.metastore.ds.connection.url.hook：查找JDO连接url时hook的名字，默认是javax.jdo.option.ConnectionURL；</span><br><span class="line"></span><br><span class="line">hive.metastore.ds.retry.attempts：当出现连接错误时重试连接的次数，默认是1次；</span><br><span class="line"></span><br><span class="line">hive.metastore.ds.retry.interval：metastore重试连接的间隔时间，默认1000毫秒；</span><br><span class="line"></span><br><span class="line">hive.metastore.server.min.threads：在thrift服务池中最小的工作线程数，默认是200；</span><br><span class="line"></span><br><span class="line">hive.metastore.server.max.threads：最大线程数，默认是100000；</span><br><span class="line"></span><br><span class="line">hive.metastore.server.tcp.keepalive：metastore的server是否开启长连接，长连可以预防半连接的积累，默认是true；</span><br><span class="line"></span><br><span class="line">hive.metastore.sasl.enabled：metastore thrift接口的安全策略，开启则用SASL加密接口，客户端必须要用Kerberos机制鉴权，默认是不开启false；</span><br><span class="line"></span><br><span class="line">hive.metastore.kerberos.keytab.file：在开启sasl后kerberos的keytab文件存放路径，默认是空；</span><br><span class="line"></span><br><span class="line">hive.metastore.kerberos.principal：kerberos的principal，_HOST部分会动态替换，默认是hive-metastore/_HOST@EXAMPLE.COM；</span><br><span class="line"></span><br><span class="line">hive.metastore.cache.pinobjtypes：在cache中支持的metastore的对象类型，由逗号分隔，默认是Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order；</span><br><span class="line"></span><br><span class="line">hive.metastore.authorization.storage.checks：在做类似drop partition操作时，metastore是否要认证权限，默认是false；</span><br><span class="line"></span><br><span class="line">hive.metastore.schema.verification：强制metastore的schema一致性，开启的话会校验在metastore中存储的信息的版本和hive的jar包中的版本一致性，并且关闭自动schema迁移，用户必须手动的升级hive并且迁移schema，关闭的话只会在版本不一致时给出警告，默认是false不开启；</span><br><span class="line"></span><br><span class="line">hive.index.compact.file.ignore.hdfs：在索引文件中存储的hdfs地址将在运行时被忽略，如果开启的话；如果数据被迁移，那么索引文件依然可用，默认是false；</span><br><span class="line"></span><br><span class="line">hive.optimize.index.filter.compact.minsize：压缩索引自动应用的最小输入大小，默认是5368709120；</span><br><span class="line"></span><br><span class="line">hive.optimize.index.filter.compact.maxsize：同上，相反含义，如果是负值代表正无穷，默认是-1；</span><br><span class="line"></span><br><span class="line">hive.index.compact.query.max.size：一个使用压缩索引做的查询能取到的最大数据量，默认是10737418240 个byte；负值代表无穷大；</span><br><span class="line"></span><br><span class="line">hive.index.compact.query.max.entries：使用压缩索引查询时能读到的最大索引项数，默认是10000000；负值代表无穷大；</span><br><span class="line"></span><br><span class="line">hive.index.compact.binary.search：在索引表中是否开启二分搜索进行索引项查询，默认是true；</span><br><span class="line"></span><br><span class="line">hive.exec.concatenate.check.index：如果设置为true，那么在做ALTER TABLE tbl_name CONCATENATE on a table/partition（有索引） 操作时，抛出错误；可以帮助用户避免index的删除和重建；</span><br><span class="line"></span><br><span class="line">hive.stats.dbclass：存储hive临时统计信息的数据库，默认是jdbc:derby；</span><br><span class="line"></span><br><span class="line">hive.stats.autogather：在insert overwrite命令时自动收集统计信息，默认开启true；</span><br><span class="line"></span><br><span class="line">hive.stats.jdbcdriver：数据库临时存储hive统计信息的jdbc驱动；</span><br><span class="line"></span><br><span class="line">hive.stats.dbconnectionstring：临时统计信息数据库连接串，默认jdbc:derby:databaseName=TempStatsStore;create=true；</span><br><span class="line"></span><br><span class="line">hive.stats.defaults.publisher：如果dbclass不是jdbc或者hbase，那么使用这个作为默认发布，必须实现StatsPublisher接口，默认是空；</span><br><span class="line"></span><br><span class="line">hive.stats.defaults.aggregator：如果dbclass不是jdbc或者hbase，那么使用该类做聚集，要求实现StatsAggregator接口，默认是空；</span><br><span class="line"></span><br><span class="line">hive.stats.jdbc.timeout：jdbc连接超时配置，默认30秒；</span><br><span class="line"></span><br><span class="line">hive.stats.retries.max：当统计发布合聚集在更新数据库时出现异常时最大的重试次数，默认是0，不重试；</span><br><span class="line"></span><br><span class="line">hive.stats.retries.wait：重试次数之间的等待窗口，默认是3000毫秒；</span><br><span class="line"></span><br><span class="line">hive.client.stats.publishers：做count的job的统计发布类列表，由逗号隔开，默认是空；必须实现org.apache.hadoop.hive.ql.stats.ClientStatsPublisher接口；</span><br><span class="line"></span><br><span class="line">hive.client.stats.counters：没什么用~~~</span><br><span class="line"></span><br><span class="line">hive.security.authorization.enabled：hive客户端是否认证，默认是false；</span><br><span class="line"></span><br><span class="line">hive.security.authorization.manager：hive客户端认证的管理类，默认是org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider；用户定义的要实现org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider；</span><br><span class="line"></span><br><span class="line">hive.security.authenticator.manager：hive客户端授权的管理类，默认是org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator；用户定义的需要实现org.apache.hadoop.hive.ql.security.HiveAuthenticatorProvider；</span><br><span class="line"></span><br><span class="line">hive.security.authorization.createtable.user.grants：当表创建时自动授权给用户，默认是空；</span><br><span class="line"></span><br><span class="line">hive.security.authorization.createtable.group.grants：同上，自动授权给组，默认是空；</span><br><span class="line"></span><br><span class="line">hive.security.authorization.createtable.role.grants：同上，自动授权给角色，默认是空；</span><br><span class="line"></span><br><span class="line">hive.security.authorization.createtable.owner.grants：同上，自动授权给owner，默认是空；</span><br><span class="line"></span><br><span class="line">hive.security.metastore.authorization.manager：metastore的认证管理类，默认是org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider；用户定义的必须实现org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider接口；接口参数要包含org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider接口；使用HDFS的权限控制认证而不是hive的基于grant的方式；</span><br><span class="line"></span><br><span class="line">hive.security.metastore.authenticator.manager：metastore端的授权管理类，默认是org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator，自定义的必须实现org.apache.hadoop.hive.ql.security.HiveAuthenticatorProvider接口；</span><br><span class="line"></span><br><span class="line">hive.metastore.pre.event.listeners：在metastore做数据库任何操作前执行的事件监听类列表；</span><br><span class="line"></span><br><span class="line">fs.har.impl：访问Hadoop Archives的实现类，低于hadoop 0.20版本的都不兼容，默认是org.apache.hadoop.hive.shims.HiveHarFileSystem；</span><br><span class="line"></span><br><span class="line">hive.archive.enabled：是否允许归档操作，默认是false；</span><br><span class="line"></span><br><span class="line">hive.archive.har.parentdir.settable：在创建HAR文件时必须要有父目录，需要手动设置，在新的hadoop版本会支持，默认是false；</span><br><span class="line"></span><br><span class="line">hive.support.concurrency：hive是否支持并发，默认是false，支持读写锁的话，必须要起zookeeper；</span><br><span class="line"></span><br><span class="line">hive.lock.mapred.only.operation：控制是否在查询时加锁，默认是false；</span><br><span class="line"></span><br><span class="line">hive.lock.numretries：获取锁时尝试的重试次数，默认是100；</span><br><span class="line"></span><br><span class="line">hive.lock.sleep.between.retries：在重试间隔的睡眠时间，默认60秒；</span><br><span class="line"></span><br><span class="line">hive.zookeeper.quorum：zk地址列表，默认是空；</span><br><span class="line"></span><br><span class="line">hive.zookeeper.client.port：zk服务器的连接端口，默认是2181；</span><br><span class="line"></span><br><span class="line">hive.zookeeper.session.timeout：zk客户端的session超时时间，默认是600000；</span><br><span class="line"></span><br><span class="line">hive.zookeeper.namespace：在所有zk节点创建后的父节点，默认是hive_zookeeper_namespace；</span><br><span class="line"></span><br><span class="line">hive.zookeeper.clean.extra.nodes：在session结束时清除所有额外node；</span><br><span class="line"></span><br><span class="line">hive.cluster.delegation.token.store.class：代理token的存储实现类，默认是org.apache.hadoop.hive.thrift.MemoryTokenStore，可以设置为org.apache.hadoop.hive.thrift.ZooKeeperTokenStore来做负载均衡集群；</span><br><span class="line"></span><br><span class="line">hive.cluster.delegation.token.store.zookeeper.connectString：zk的token存储连接串，默认是localhost:2181；</span><br><span class="line"></span><br><span class="line">hive.cluster.delegation.token.store.zookeeper.znode：token存储的节点跟路径，默认是/hive/cluster/delegation；</span><br><span class="line"></span><br><span class="line">hive.cluster.delegation.token.store.zookeeper.acl：token存储的ACL，默认是sasl:hive/host1@example.com:cdrwa,sasl:hive/host2@example.com:cdrwa；</span><br><span class="line"></span><br><span class="line">hive.use.input.primary.region：从一张input表创建表时，创建这个表到input表的主region，默认是true；</span><br><span class="line"></span><br><span class="line">hive.default.region.name：默认region的名字，默认是default；</span><br><span class="line"></span><br><span class="line">hive.region.properties：region的默认的文件系统和jobtracker，默认是空；</span><br><span class="line"></span><br><span class="line">hive.cli.print.header：查询输出时是否打印名字和列，默认是false；</span><br><span class="line"></span><br><span class="line">hive.cli.print.current.db：hive的提示里是否包含当前的db，默认是false；</span><br><span class="line"></span><br><span class="line">hive.hbase.wal.enabled：写入hbase时是否强制写wal日志，默认是true；</span><br><span class="line"></span><br><span class="line">hive.hwi.war.file：hive在web接口是的war文件的路径，默认是lib/hive-hwi-xxxx(version).war；</span><br><span class="line"></span><br><span class="line">hive.hwi.listen.host：hwi监听的host地址，默认是0.0.0.0；</span><br><span class="line"></span><br><span class="line">hive.hwi.listen.port：hwi监听的端口，默认是9999；</span><br><span class="line"></span><br><span class="line">hive.test.mode：hive是否运行在测试模式，默认是false；</span><br><span class="line"></span><br><span class="line">hive.test.mode.prefix：在测试模式运行时，表的前缀字符串，默认是test_；</span><br><span class="line"></span><br><span class="line">hive.test.mode.samplefreq：如果hive在测试模式运行，并且表未分桶，抽样频率是多少，默认是32；</span><br><span class="line"></span><br><span class="line">hive.test.mode.nosamplelist：在测试模式运行时不进行抽样的表列表，默认是空；</span><br></pre></td></tr></table></figure>
<h2 id="12、HIVE_数据倾斜的可能原因有哪些？主要解决方法有哪些？">12、HIVE 数据倾斜的可能原因有哪些？主要解决方法有哪些？</h2><h3 id="数据倾斜的原因">数据倾斜的原因</h3><ul>
<li><p>操作<br><img src="http://7xigvj.com1.z0.glb.clouddn.com/15311306228352.jpg" alt=""></p>
</li>
<li><p>原因</p>
<ol>
<li>key分布不均匀</li>
<li>业务数据本身的特性</li>
<li>建表时考虑不周</li>
<li>某些SQL语句本身就有数据倾斜</li>
</ol>
</li>
<li><p>表现<br>  任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。</p>
</li>
</ul>
<p>单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。 最长时长远大于平均时长。</p>
<h3 id="数据倾斜的解决方案">数据倾斜的解决方案</h3><ul>
<li>参数调节</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive.map.aggr=true</span><br><span class="line"></span><br><span class="line">Map 端部分聚合，相当于Combiner</span><br><span class="line"></span><br><span class="line">hive.groupby.skewindata=true</span><br><span class="line"></span><br><span class="line">有数据倾斜的时候进行负载均衡，当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。</span><br></pre></td></tr></table></figure>
<ul>
<li><p>SQL语句调节</p>
<ul>
<li><p>如何Join：</p>
<p>关于驱动表的选取，选用join key分布最均匀的表作为驱动表</p>
<p>做好列裁剪和filter操作，以达到两表做join的时候，数据量相对变小的效果。</p>
</li>
<li><p>大小表Join：</p>
<p>使用map join让小的维度表（1000条以下的记录条数） 先进内存。在map端完成reduce.</p>
</li>
<li><p>大表Join大表：</p>
<p>把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。</p>
</li>
<li><p>count distinct大量相同特殊值</p>
<p>count distinct时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。</p>
</li>
<li><p>group by维度过小：</p>
<p>采用sum() group by的方式来替换count(distinct)完成计算。</p>
</li>
<li><p>特殊情况特殊处理：</p>
<p>在业务逻辑优化效果的不大情况下，有些时候是可以将倾斜的数据单独拿出来处理。最后union回去。</p>
</li>
</ul>
</li>
</ul>
<h3 id="典型的业务场景">典型的业务场景</h3><ul>
<li>空值产生的数据倾斜</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">场景：如日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和 用户表中的user_id 关联，会碰到数据倾斜的问题。</span><br><span class="line"></span><br><span class="line">解决方法1： user_id为空的不参与关联（红色字体为修改后）</span><br><span class="line"></span><br><span class="line">复制代码</span><br><span class="line">select * from log a</span><br><span class="line">  join users b</span><br><span class="line">  on a.user_id is not null</span><br><span class="line">  and a.user_id = b.user_id</span><br><span class="line">union all</span><br><span class="line">select * from log a</span><br><span class="line">  where a.user_id is null;</span><br><span class="line">复制代码</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">解决方法2 ：赋与空值分新的key值</span><br><span class="line"></span><br><span class="line">select *</span><br><span class="line">  from log a</span><br><span class="line">  left outer join users b</span><br><span class="line">  on case when a.user_id is null then concat(‘hive’,rand() ) else a.user_id end = b.user_id;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">结论：方法2比方法1效率更好，不但io少了，而且作业数也少了。解决方法1中 log读取两次，jobs是2。解决方法2 job数是1 。这个优化适合无效 id (比如 -99 , ’’, null 等) 产生的倾斜问题。把空值的 key 变成一个字符串加上随机数，就能把倾斜的数据分到不同的reduce上 ,解决数据倾斜问题。</span><br></pre></td></tr></table></figure>
<ul>
<li>不同数据类型关联产生数据倾斜</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">场景：用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的Join操作时，默认的Hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reducer中。</span><br><span class="line"></span><br><span class="line">解决方法：把数字类型转换成字符串类型</span><br><span class="line"></span><br><span class="line">select * from users a</span><br><span class="line">  left outer join logs b</span><br><span class="line">  on a.usr_id = cast(b.user_id as string)</span><br></pre></td></tr></table></figure>
<ul>
<li>小表不小不大，怎么用 map join 解决倾斜问题</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常高，但如果小表很大，大到map join会出现bug或异常，这时就需要特别的处理。 以下例子:</span><br><span class="line"></span><br><span class="line">select * from log a</span><br><span class="line">  left outer join users b</span><br><span class="line">  on a.user_id = b.user_id;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">users 表有 600w+ 的记录，把 users 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。</span><br><span class="line"></span><br><span class="line">解决方法：</span><br><span class="line"></span><br><span class="line">复制代码</span><br><span class="line">select /*+mapjoin(x)*/* from log a</span><br><span class="line">  left outer join (</span><br><span class="line">    select  /*+mapjoin(c)*/d.*</span><br><span class="line">      from ( select distinct user_id from log ) c</span><br><span class="line">      join users d</span><br><span class="line">      on c.user_id = d.user_id</span><br><span class="line">    ) x</span><br><span class="line">  on a.user_id = b.user_id;</span><br><span class="line"> </span><br><span class="line">复制代码</span><br><span class="line">假如，log里user_id有上百万个，这就又回到原来map join问题。所幸，每日的会员uv不会太多，有交易的会员不会太多，有点击的会员不会太多，有佣金的会员不会太多等等。所以这个方法能解决很多场景下的数据倾斜问题。</span><br></pre></td></tr></table></figure>
<h3 id="总结">总结</h3><p>使map的输出数据更均匀的分布到reduce中去，是我们的最终目标。由于Hash算法的局限性，按key Hash会或多或少的造成数据倾斜。大量经验表明数据倾斜的原因是人为的建表疏忽或业务逻辑可以规避的。在此给出较为通用的步骤：</p>
<p>1、采样log表，哪些user_id比较倾斜，得到一个结果表tmp1。由于对计算框架来说，所有的数据过来，他都是不知道数据分布情况的，所以采样是并不可少的。</p>
<p>2、数据的分布符合社会学统计规则，贫富不均。倾斜的key不会太多，就像一个社会的富人不多，奇特的人不多一样。所以tmp1记录数会很少。把tmp1和users做map join生成tmp2,把tmp2读到distribute file cache。这是一个map过程。</p>
<p>3、map读入users和log，假如记录来自log,则检查user_id是否在tmp2里，如果是，输出到本地文件a,否则生成<user_id,value>的key,value对，假如记录来自member,生成<user_id,value>的key,value对，进入reduce阶段。</user_id,value></user_id,value></p>
<p>4、最终把a文件，把Stage3 reduce阶段输出的文件合并起写到hdfs。</p>
<p>如果确认业务需要这样倾斜的逻辑，考虑以下的优化方案：</p>
<p>1、对于join，在判断小表不大于1G的情况下，使用map join</p>
<p>2、对于group by或distinct，设定 hive.groupby.skewindata=true</p>
<p>3、尽量使用上述的SQL语句调节进行优化</p>
<h2 id="13、数据仓库之数据架构设计图，及每个模块的主要作用？">13、数据仓库之数据架构设计图，及每个模块的主要作用？</h2><p><img src="http://7xigvj.com1.z0.glb.clouddn.com/15311497429809.jpg" alt=""></p>
<p><img src="http://7xigvj.com1.z0.glb.clouddn.com/15311497630184.jpg" alt=""></p>
<p><img src="http://7xigvj.com1.z0.glb.clouddn.com/15311498682475.jpg" alt=""></p>
<h2 id="14、利用_HiveSQL_语句，创建如下两张表：">14、利用 HiveSQL 语句，创建如下两张表：</h2><p>创建员工基本信息表(EmployeeInfo)，字段包括(员工 ID，员工姓名，员工身份证号，性别，年龄，所属部门，岗位，入职公司时间，离职公司时间)，分区字段为入职公司时间，其行分隔符为”\n “，字段分隔符为”\t “。其中所属部门包括行政部、财务部、研发部、教学部，其对应岗位包括行政经理、行政专员、财务经理、财务专员、研发工程师、测试工程师、实施工程师、讲师、助教、班主任等，时间类型值如：2018-05-10 11:00:00</p>
<p>创建员工收入表(IncomeInfo)，字段包括(员工 ID，员工姓名，收入金额，收入所属</p>
<p>月份，收入类型，收入薪水的时间)，分区字段为发放薪水的时间，其中收入类型包括薪资、</p>
<p>奖金、公司福利、罚款四种情况 ; 时间类型值如：2018-05-10 11:00:00。</p>
<h2 id="15、用_HQL_实现，求公司每年的员工费用总支出各是多少，并按年份降序排列?">15、用 HQL 实现，求公司每年的员工费用总支出各是多少，并按年份降序排列?</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select sum(i_price), to_year(i_date) as &apos;c_year&apos; from IncomeInfo</span><br><span class="line"> group by c_year order by c_year desc;</span><br></pre></td></tr></table></figure>
<h2 id="16、用_HQL_实现，求各部门每年的员工费用总支出各是多少，并按年份降序，按部门的支出升序排列？">16、用 HQL 实现，求各部门每年的员工费用总支出各是多少，并按年份降序，按部门的支出升序排列？</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select sum(i_price), to_year(i_date) as &apos;c_year&apos; from IncomeInfo left join Emp on uid = uid</span><br><span class="line"> group by c_year order by c_year desc, eid asc;</span><br></pre></td></tr></table></figure>
<h2 id="17、用_HQL_实现，求各部门历史所有员工费用总支出各是多少，按总支出多少排名降序，遇到值相等情况，不留空位。">17、用 HQL 实现，求各部门历史所有员工费用总支出各是多少，按总支出多少排名降序，遇到值相等情况，不留空位。</h2><h2 id="18、用_HQL_实现，创建并生成员工薪资收入动态变化表，即员工_ID，员工姓名，员工本月薪资，本月薪资发放时间，员工上月薪资，上月薪资发放时间。分区字段为本月薪资发放时间。">18、用 HQL 实现，创建并生成员工薪资收入动态变化表，即员工 ID，员工姓名，员工本月薪资，本月薪资发放时间，员工上月薪资，上月薪资发放时间。分区字段为本月薪资发放时间。</h2><h2 id="19、用_HQL_实现，薪资涨幅方面，2018_年_5_月份谁的工资涨的最多，谁的涨幅最大？">19、用 HQL 实现，薪资涨幅方面，2018 年 5 月份谁的工资涨的最多，谁的涨幅最大？</h2><h2 id="20、对象二分查找的实现。">20、对象二分查找的实现。</h2><p>有学生基本信息类 Student，包括字段学号，姓名，班级，入学日期，共四个字段。其中学号是 SXXX 的格式，如 S001,S002 等。</p>
<p>现给定 6 个学生对象，如:<br>(S100,’张一”,”计科 1 班”,20180903),<br>(”S110”, ”张二”,”计科 1 班”,20180903),<br>(”S090”, ”张三”,”计科 2 班”,20180830),<br>(”S080”, ”张四”,”计科 2 班”,20180904),<br>(”S070”, ”张五”,”计科 2 班”,20180901),<br>(”S101”, ”张六”,”计科 1 班”,20180902),</p>
<p>求给定学号”S101”，通过对以上对象集进行二分查找后，确定是否在已给定的学生对象中，</p>
<p>若存在，则打印该学生信息，若不存在，则输出”查无此人”。</p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/bigdata/" rel="tag"># bigdata</a>
          
            <a href="/tags/hive/" rel="tag"># hive</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/05/2018-07-05-1/" rel="next" title="常见数据结构与算法整理总结（下）">
                <i class="fa fa-chevron-left"></i> 常见数据结构与算法整理总结（下）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/06/2018-07-06/" rel="prev" title="转载 -《管理自己》-《Managing Oneself》">
                转载 -《管理自己》-《Managing Oneself》 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC80MjM5OS8xODk0Ng=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://avatars0.githubusercontent.com/u/3776267"
                alt="Aaron OuYang"/>
            
              <p class="site-author-name" itemprop="name">Aaron OuYang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">10</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">分类</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">19</span>
                    <span class="site-state-item-name">标签</span>
                  
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Hive_笔试题"><span class="nav-number">1.</span> <span class="nav-text">Hive 笔试题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-_Hive_的架构设计与运行流程，及其各模块的主要作用是什么，请画出架构图"><span class="nav-number">1.1.</span> <span class="nav-text">1.  Hive 的架构设计与运行流程，及其各模块的主要作用是什么，请画出架构图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-_Hive_的数据模型组成，及各组成模块的应用场景，请简要描述"><span class="nav-number">1.2.</span> <span class="nav-text">2. Hive 的数据模型组成，及各组成模块的应用场景，请简要描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-_Hive_支持的文件格式和压缩格式，及其各自的特点？"><span class="nav-number">1.3.</span> <span class="nav-text">3.  Hive 支持的文件格式和压缩格式，及其各自的特点？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-_Hive_内外表的区分方法，及内外表的差异点？"><span class="nav-number">1.4.</span> <span class="nav-text">4.  Hive 内外表的区分方法，及内外表的差异点？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-_Hive_视图如何创建，视图有什么特点，及其应用场景？"><span class="nav-number">1.5.</span> <span class="nav-text">5. Hive 视图如何创建，视图有什么特点，及其应用场景？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-_Hive_常用的_12_个命令，及其作用"><span class="nav-number">1.6.</span> <span class="nav-text">6. Hive 常用的 12 个命令，及其作用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-_Hive_常用的_10_个系统函数，及其作用"><span class="nav-number">1.7.</span> <span class="nav-text">7. Hive 常用的 10 个系统函数，及其作用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-_请详细描述将一个有结构的文本文件_student-txt_导入到一个_Hive_表当中的步骤，及其关键字？"><span class="nav-number">1.8.</span> <span class="nav-text">8. 请详细描述将一个有结构的文本文件 student.txt 导入到一个 Hive 表当中的步骤，及其关键字？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-_请简述_udf/udaf/udtf_是什么，各自解决的问题，及典型代表应用场景。"><span class="nav-number">1.9.</span> <span class="nav-text">9. 请简述 udf/udaf/udtf 是什么，各自解决的问题，及典型代表应用场景。</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-_udaf_的实现步骤，及其包含的主要方法，及每个方法要解决的问题，并写代码自实现聚合函数_max_函数？"><span class="nav-number">1.10.</span> <span class="nav-text">10. udaf 的实现步骤，及其包含的主要方法，及每个方法要解决的问题，并写代码自实现聚合函数 max 函数？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11、_hive_设置参数的方法有哪些？并列举_8_个常用的参数设置？"><span class="nav-number">1.11.</span> <span class="nav-text">11、 hive 设置参数的方法有哪些？并列举 8 个常用的参数设置？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12、HIVE_数据倾斜的可能原因有哪些？主要解决方法有哪些？"><span class="nav-number">1.12.</span> <span class="nav-text">12、HIVE 数据倾斜的可能原因有哪些？主要解决方法有哪些？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据倾斜的原因"><span class="nav-number">1.12.1.</span> <span class="nav-text">数据倾斜的原因</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据倾斜的解决方案"><span class="nav-number">1.12.2.</span> <span class="nav-text">数据倾斜的解决方案</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#典型的业务场景"><span class="nav-number">1.12.3.</span> <span class="nav-text">典型的业务场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">1.12.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13、数据仓库之数据架构设计图，及每个模块的主要作用？"><span class="nav-number">1.13.</span> <span class="nav-text">13、数据仓库之数据架构设计图，及每个模块的主要作用？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#14、利用_HiveSQL_语句，创建如下两张表："><span class="nav-number">1.14.</span> <span class="nav-text">14、利用 HiveSQL 语句，创建如下两张表：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#15、用_HQL_实现，求公司每年的员工费用总支出各是多少，并按年份降序排列?"><span class="nav-number">1.15.</span> <span class="nav-text">15、用 HQL 实现，求公司每年的员工费用总支出各是多少，并按年份降序排列?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#16、用_HQL_实现，求各部门每年的员工费用总支出各是多少，并按年份降序，按部门的支出升序排列？"><span class="nav-number">1.16.</span> <span class="nav-text">16、用 HQL 实现，求各部门每年的员工费用总支出各是多少，并按年份降序，按部门的支出升序排列？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#17、用_HQL_实现，求各部门历史所有员工费用总支出各是多少，按总支出多少排名降序，遇到值相等情况，不留空位。"><span class="nav-number">1.17.</span> <span class="nav-text">17、用 HQL 实现，求各部门历史所有员工费用总支出各是多少，按总支出多少排名降序，遇到值相等情况，不留空位。</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#18、用_HQL_实现，创建并生成员工薪资收入动态变化表，即员工_ID，员工姓名，员工本月薪资，本月薪资发放时间，员工上月薪资，上月薪资发放时间。分区字段为本月薪资发放时间。"><span class="nav-number">1.18.</span> <span class="nav-text">18、用 HQL 实现，创建并生成员工薪资收入动态变化表，即员工 ID，员工姓名，员工本月薪资，本月薪资发放时间，员工上月薪资，上月薪资发放时间。分区字段为本月薪资发放时间。</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#19、用_HQL_实现，薪资涨幅方面，2018_年_5_月份谁的工资涨的最多，谁的涨幅最大？"><span class="nav-number">1.19.</span> <span class="nav-text">19、用 HQL 实现，薪资涨幅方面，2018 年 5 月份谁的工资涨的最多，谁的涨幅最大？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#20、对象二分查找的实现。"><span class="nav-number">1.20.</span> <span class="nav-text">20、对象二分查找的实现。</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Aaron OuYang</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>




  
  <script src="/js/src/scrollspy.js?v=6.7.0"></script>
<script src="/js/src/post-details.js?v=6.7.0"></script>



  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  


  
    <script>
  window.livereOptions = {
    refer: '2018/07/06/2018-07-09/'
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
</script>

  


  





  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
